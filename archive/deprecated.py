# Deprecated code for Large Graph Path Finder project.
# Retained for reference; not used in current implementation.

import matplotlib.pyplot as plt
import networkx as nx
from datetime import datetime
from typing import List, Tuple
from core.parser import load_graph, GraphParseError
from core import pathfinder
from core import graphing
from typing import Dict, cast
import time, logging, psutil, os, json, threading, warnings
from collections import deque, defaultdict
from typing import Dict, List, Tuple, Callable, Optional
from core import algorithms as alg
from core import landmarks
from typing import TypedDict

def memory_mb() -> float:
    """
    Returns the current process memory usage in MB.
    """
    process = psutil.Process(os.getpid())
    mem_bytes = process.memory_info().rss
    return mem_bytes / (1024 * 1024)

def timer_worker(stop_flag: list, duration_sec: int = 60):
    """
    Count-up timer.
    Runs until preprocessing is finished (stop_flag[0] = True).
    Logs and prints every second.
    Warns if time > duration_sec or memory > 1024 MB,
    but does not stop early.
    """
    elapsed = 0
    while not stop_flag[0]:
        mem = memory_mb()
        msg = f"[TIMER] {elapsed:>3d}s elapsed | Memory: {mem:.2f} MB"
        print(msg, flush=True)  # prints to terminal
        logging.info(msg)  # still log to run.log

        if elapsed >= duration_sec:
            warn = f"[TIMER] Time budget exceeded ({elapsed}s > {duration_sec}s)"
            print(f"[WARN] {warn}", flush=True)
            logging.warning(warn)
        if mem > 1024.0:
            warn = f"[TIMER] Memory usage exceeded: {mem:.2f} MB > 1024 MB"
            print(f"[WARN] {warn}", flush=True)
            logging.warning(warn)

        time.sleep(1)
        elapsed += 1

    msg = f"[TIMER] Preprocessing finished at {elapsed}s, stopping timer."
    print(msg, flush=True)
    logging.info(msg)


def deprecated_code():
    # originally started in main.py, moved here for clarity
    # Before preprocessing
    stop_flag = [False]
    t = threading.Thread(target=timer_worker, args=(stop_flag, 60), daemon=True)
    t.start()

    # Cast for static type checker
    weight_map = cast(Dict[int, Dict[int, float]], weight_map)
    stop_flag[0] = True  # signal timer to stop

    # 3) Parse & preprocess
    try:
        adj = load_graph(gpath)
    except GraphParseError as e:
        print(f"[ERROR] Failed to load graph: {e}")
        logging.error(f"Graph parse error: {e}")
        return

    # Build indexes for reachability + weight lookup
    index = pathfinder.preprocess_index(adj)


#  parser.py
def load_graph(json_file):
    """
    Parse a JSON graph file generated by graph-generator.py.

    Parameters
    ----------
    json_file : str
        Path to the JSON file.

    Returns
    -------
    dict
        Adjacency list representation: node -> list of (neighbor, weight).

    Raises
    ------
    GraphParseError
        If the file does not exist, is malformed, or missing required fields.
    """
    if not os.path.exists(json_file):
        raise GraphParseError(f"File not found: {json_file}")

    try:
        with open(json_file, "r") as f:
            data = json.load(f)
    except json.JSONDecodeError as e:
        raise GraphParseError(f"Invalid JSON in {json_file}: {e}")

    # Validate required keys
    if "nodes" not in data or "links" not in data:
        raise GraphParseError("JSON missing required 'nodes' or 'links' sections")

    # Build adjacency list
    adj = {node["id"]: [] for node in data["nodes"]}
    directed = data.get("directed", False)

    for edge in data["links"]:
        if "source" not in edge or "target" not in edge or "weight" not in edge:
            raise GraphParseError("Edge entry missing 'source', 'target', or 'weight'")
        u, v, w = edge["source"], edge["target"], edge["weight"]
        adj[u].append((v, w))
        if not directed:
            adj[v].append((u, w))  # Add reverse edge for undirected graphs

    return adj


# pathfinder.py
# ---------------- Preprocessing ----------------
def preprocess_index(adj: Adjacency) -> Index:
    """
    Build reusable indexes for queries:
      - weight_map: fast edge weight lookup
      - weak_components: quick reachability check
      - basic stats (node_count, edge_count, uniform_weights)
      - optional: landmarks + ALT distance tables for large graphs
    """
    weight_map = _build_weight_map(adj)
    weak_components = _build_weak_components(adj)

    weights = [w for edges in adj.values() for _, w in edges]
    uniform = all((w == 1 or w == 1.0) for w in weights) if weights else True

    edge_count = len(weights)
    node_count = len(adj)

    index: Index = {
        "weight_map": weight_map,
        "weak_components": weak_components,
        "uniform_weights": uniform,
        "node_count": node_count,
        "edge_count": edge_count,
    }

    # ALT heuristic precomputation for very large graphs
    if node_count >= 10000:
        L = landmarks.select_landmarks(adj, k=12)
        dist_L_to_v, dist_v_to_L = landmarks.build_tables(adj, L)
        index.update(
            {
                "landmarks": L,
                "dist_L_to_v": dist_L_to_v,
                "dist_v_to_L": dist_v_to_L,
            }
        )

    return index
