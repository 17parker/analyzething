"""
main.py - Driver for Large Graph Path Finder (LGPF)

Flow:
1) Ask user for a JSON graph file inside input/.
2) Start a 60s countdown (LOGS ONLY) that covers preprocessing.
3) Parse & preprocess (adjacency, indexes; visualize small graphs).
4) Ask for a query file inside input/, OR let the user enter pairs interactively.
5) Choose algorithm once via pathfinder.choose_algorithm(adj, index).
6) Run all queries, print to console, write to output/results.txt, log to output/run.log.
7) Warn if preprocessing exceeded 60s or memory > 1GB.

Note: For small graphs we aim for accuracy; for large, future work will cluster
(e.g., "Tokyo center" estimate) to trade precision for speed.
"""

import matplotlib.pyplot as plt
import networkx as nx
from datetime import datetime
from typing import List, Tuple
from parser import load_graph, GraphParseError
from pathfinder import *
from graphing import *
from typing import Dict, cast
import time, logging, psutil, os, json, threading, warnings

# Silence noisy numpy warning on some distros
warnings.filterwarnings("ignore", category=UserWarning, module="numpy._core.getlimits")


# ---------------- Constants ----------------
INPUT_DIR = "input"
OUTPUT_DIR = "output"


# ---------------- Logging Setup ----------------
def setup_logging():
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    logging.basicConfig(
        filename=os.path.join(OUTPUT_DIR, "run.log"),
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] %(message)s",
    )
    logging.info("=== Program Started ===")


# ---------------- Utilities ----------------
def memory_mb() -> float:
    """Current process RSS in MB."""
    proc = psutil.Process(os.getpid())
    return proc.memory_info().rss / (1024 * 1024)


def timer_worker(stop_flag: dict, duration_sec: int = 60, mem_limit_mb: float = 1024.0):
    """
    Count-up timer with abort capability.
    Stops when stop_flag["done"] is set True.
    If elapsed >= duration_sec or memory > mem_limit_mb,
    sets stop_flag["abort"] = True so preprocessing can stop.
    """
    elapsed = 0
    while not stop_flag["done"]:
        mem = memory_mb()
        msg = f"[TIMER] {elapsed:>3d}s elapsed | Memory: {mem:.2f} MB"
        print(msg, flush=True)
        logging.info(msg)

        if elapsed >= duration_sec:
            warn = f"[TIMER] Time budget exceeded ({elapsed}s > {duration_sec}s)"
            print(f"[WARN] {warn}", flush=True)
            logging.warning(warn)
            stop_flag["abort"] = True

        if mem > mem_limit_mb:
            warn = f"[TIMER] Memory usage exceeded: {mem:.2f} MB > {mem_limit_mb} MB"
            print(f"[WARN] {warn}", flush=True)
            logging.warning(warn)
            stop_flag["abort"] = True

        time.sleep(1)
        elapsed += 1

    # msg = f"[TIMER] Stopped at {elapsed}s"
    # print(msg, flush=True)
    # logging.info(msg)


# ---------------- Prompt Helpers ----------------
def _list_jsons_in_input() -> List[str]:
    os.makedirs(INPUT_DIR, exist_ok=True)
    return sorted([f for f in os.listdir(INPUT_DIR) if f.lower().endswith(".json")])


def _list_files_in_input() -> List[str]:
    os.makedirs(INPUT_DIR, exist_ok=True)
    return sorted(
        [f for f in os.listdir(INPUT_DIR) if os.path.isfile(os.path.join(INPUT_DIR, f))]
    )


def get_graph_file() -> Tuple[str, str]:
    """
    Prompt user for a JSON graph file inside input/.
    If only one .json exists, pressing Enter selects it.
    """
    candidates = _list_jsons_in_input()
    if candidates:
        print(f"[INFO] JSON files in {INPUT_DIR}/: {', '.join(candidates)}")
    default = candidates[0] if len(candidates) == 1 else None
    hint = f" [Enter for {default}]" if default else ""

    while True:
        try:
            fname = input(
                f"Enter graph filename (inside {INPUT_DIR}/, *.json){hint}: "
            ).strip()
        except EOFError:
            if default:
                fname = default
            else:
                print("[ERROR] No TTY input and no default JSON available.")
                raise
        if not fname and default:
            fname = default
        path = os.path.join(INPUT_DIR, fname)
        if not fname or not os.path.exists(path):
            print(f"[WARN] File not found in {INPUT_DIR}/. Try again.")
            continue
        if not fname.lower().endswith(".json"):
            print("[WARN] Input must be a .json file generated by graph-generator.py.")
            continue
        return fname, path


def get_query_file_or_interactive() -> List[Tuple[int, int]]:
    """
    Ask for a query filename OR let the user enter pairs interactively.
    Returns a list of (src, dst) integer pairs.
    """
    files = _list_files_in_input()
    if files:
        print(f"[INFO] Files in {INPUT_DIR}/: {', '.join(files)}")

    choice = input(
        f"Enter query filename (inside {INPUT_DIR}/), or press Enter to input pairs interactively: "
    ).strip()

    if choice:
        qpath = os.path.join(INPUT_DIR, choice)
        if not os.path.exists(qpath):
            print(f"[ERROR] Query file not found: {qpath}")
            return []
        return read_queries(qpath)

    # Interactive mode
    print(
        "[INFO] Interactive query entry. Type pairs like '0 3'. Empty line to finish."
    )
    pairs: List[Tuple[int, int]] = []
    while True:
        line = input("src dst (empty to finish): ").strip()
        if not line:
            break
        parts = line.split()
        if len(parts) != 2:
            print("[WARN] Please enter exactly two integers.")
            continue
        try:
            s, g = int(parts[0]), int(parts[1])
        except ValueError:
            print("[WARN] Both values must be integers.")
            continue
        pairs.append((s, g))
    return pairs


def read_queries(query_path: str) -> List[Tuple[int, int]]:
    """
    Read '<src> <dst>' pairs from a query file.
    Ignores blank/comment lines.
    """
    pairs: List[Tuple[int, int]] = []
    with open(query_path, "r") as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith("#"):
                continue
            parts = line.split()
            if len(parts) != 2:
                logging.warning(f"Skipping malformed line in {query_path!r}: {line!r}")
                continue
            try:
                s, g = int(parts[0]), int(parts[1])
            except ValueError:
                logging.warning(
                    f"Skipping non-integer line in {query_path!r}: {line!r}"
                )
                continue
            pairs.append((s, g))
    logging.info(f"Loaded {len(pairs)} queries from {query_path}")
    return pairs


# ---------------- Artifacts ----------------
def visualize_graph(adj: dict, out_file=os.path.join(OUTPUT_DIR, "graph.png")):
    """
    Draw the graph using NetworkX if it's small (<=100 nodes).
    Saved to output/graph.png.
    """
    G = nx.Graph()
    for u, edges in adj.items():
        for v, w in edges:
            G.add_edge(u, v, weight=w)

    if len(G.nodes) <= 100:
        pos = nx.spring_layout(G)
        edge_labels = nx.get_edge_attributes(G, "weight")
        nx.draw(G, pos, with_labels=True, node_color="lightblue", edge_color="gray")
        nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)
        plt.savefig(out_file)
        plt.close()
        logging.info(f"Graph visualization saved: {out_file}")
        print(f"[INFO] Graph visualization saved: {out_file}")
    else:
        logging.info("Graph too large to visualize (>100 nodes)")


def create_node_listing(graph_file: str, adj: dict) -> str:
    """
    Create a simple node listing JSON in output/ for reference:
    output/<graph_base>_nodes.json
    """
    nodes = list(adj.keys())
    payload = {
        "graph_file": os.path.basename(graph_file),
        "num_nodes": len(nodes),
        "nodes": nodes,
    }
    base, _ = os.path.splitext(os.path.basename(graph_file))
    out_path = os.path.join(OUTPUT_DIR, f"{base}_nodes.json")
    with open(out_path, "w") as f:
        json.dump(payload, f, indent=2)
    logging.info(f"Node listing saved: {out_path}")
    print(f"[INFO] Node listing saved: {out_path}")
    return out_path


# ---------------- Main ----------------
def main():
    setup_logging()
    print("[INFO] Starting Program...")
    # logging.info("Driver main started.")

    # 1) Ask for the graph JSON first
    gname, gpath = get_graph_file()

    # 2) Start a LOG-ONLY and times preprocessing duration
    stop_flag = {"done": False, "abort": False}
    t = threading.Thread(target=timer_worker, args=(stop_flag, 60), daemon=True)
    t.start()
    logging.info("Timer started and logging...")

    preprocess_start = time.time()

    # 3) Parse & preprocess with stop_flag awareness
    try:
        adj = load_graph(gpath, stop_flag=stop_flag)
    except GraphParseError as e:
        print(f"[ERROR] Failed to load graph: {e}")
        logging.error(f"Graph parse error: {e}")
        return

    if stop_flag["abort"]:
        print("[WARN] Preprocessing aborted due to time/memory. Using partial graph.")
        logging.warning("Preprocessing aborted due to time/memory. Using partial graph.")

    index = pathfinder.preprocess_index(adj, stop_flag=stop_flag)
    stop_flag["done"] = True  # signal timer to stop
    
    # Ensure weight_map is of type Dict[int, Dict[int, float]]
    weight_map = index.get("weight_map")
    if not isinstance(weight_map, dict):
        raise TypeError(
            "weight_map must be a dictionary of type Dict[int, Dict[int, float]]"
        )

    # Cast for static type checker
    weight_map = cast(Dict[int, Dict[int, float]], weight_map)
    # stop_flag["done"] = True  # signal timer to stop
    print(f"[INFO] Graph loaded with {len(adj)} nodes.")
    logging.info(f"Graph loaded with {len(adj)} nodes. Memory now {memory_mb():.2f} MB")

    visualize_graph(adj)
    create_node_listing(gname, adj)

    preprocess_time = time.time() - preprocess_start
    if preprocess_time > 60.0:
        msg = f"Preprocessing overtime: {preprocess_time:.2f}s (> 60s)"
        print(f"[WARN] {msg}")
        logging.warning(msg)
    else:
        logging.info(f"Preprocessing finished in {preprocess_time:.2f}s")

    # 4) Get queries: filename or interactive
    queries = get_query_file_or_interactive()
    if not queries:
        print("[WARN] No valid queries provided. Exiting.")
        logging.warning("No valid queries; exiting.")
        return

    # 5) Choose algorithm ONCE for the graph (policy; we'll add clustering later)
    algorithm = pathfinder.choose_algorithm(adj, index)
    algo_name = getattr(algorithm, "__name__", str(algorithm))
    logging.info(f"Algorithm selected by policy: {algo_name}")
    print(f"[INFO] Algorithm selected: {algo_name}")

    # 6) Run queries and write results
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    results_path = os.path.join(OUTPUT_DIR, "results.txt")

    header = [
        f"# Results {datetime.now().isoformat()} (graph={os.path.basename(gname)}, algo={algo_name})",
        "# Format: src dst | total_weight | time_sec | path_with_edge_labels",
    ]

    result_lines = []
    for s, g in queries:
        t0 = time.time()
        # Use fast reachability and policy selection
        path, _cost = pathfinder.find_path(adj, s, g, index=index)
        dt = time.time() - t0

        # Pretty output with per-edge weights + total
        line = graphing.format_result_line(s, g, path if path else [], dt, weight_map)
        print(line)
        logging.info(line)
        result_lines.append(line)

    graphing.write_results(results_path, header, result_lines)
    print(f"[INFO] Results saved: {results_path}")
    logging.info(f"Results saved: {results_path}")

    # 7) Final memory check
    mem = memory_mb()
    if mem > 1024.0:
        msg = f"Memory usage exceeded limit: {mem:.2f} MB > 1024 MB"
        print(f"[WARN] {msg}")
        logging.warning(msg)
    else:
        logging.info(f"Final memory usage: {mem:.2f} MB")

    logging.info("=== LGPF session finished ===")
    print("[INFO] Run complete. Exiting.")


if __name__ == "__main__":
    main()
